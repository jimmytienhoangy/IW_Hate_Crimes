# -*- coding: utf-8 -*-
"""Independent Work Data Prep

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1joLe2fwVcMlFYyGos46Ipu6tB-7fQaOz

# **Predicting Hate Crime Biases Using Multiclass Classification Machine Learning Models**

**Setup**

Import the necessary libraries.
"""

## for data manipulation
import pandas as pd
import numpy as np

## for plotting
import matplotlib.pyplot as plt

## for grouping sparse values
from statistics import median

## for statistical tests
import scipy

## to normalize numerical features
from sklearn.preprocessing import MinMaxScaler

## for feature selection
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, chi2

## for splitting the data
from sklearn.model_selection import train_test_split

"""# **Data Collection, Analysis, and Preparation**

Read the hate crime data into a pandas dataframe.
"""

dtf = pd.read_csv('hate_crime.csv')
dtf = dtf.sort_values(by=['incident_id'])

"""What does the raw data look like?"""

dtf.head(10)

"""What are the dimensions?"""

print("Dimensions: "+ str(dtf.shape))

"""What are the features? Do some rows have missing features?"""

dtf.info()

"""Remove preliminary features and data points that will not be needed."""

# remove duplicate rows
dtf.drop_duplicates(inplace = True)

print("Dimensions after dropping duplicate rows: "+ str(dtf.shape))

# remove columns that are duplicates or unnecessary
dtf = dtf.drop(columns=['incident_id', 'ori', 'pug_agency_name',
                        'pub_agency_unit', 'agency_type_name',
                        'state_abbr', 'population_group_code',
                        'incident_date', 'adult_victim_count',
                        'juvenile_victim_count', 'adult_offender_count',
                        'juvenile_offender_count'])

print("Dimensions after dropping columns: "+ str(dtf.shape))

"""Examine the unique values of each column and their frequencies."""

# returns a dataframe of one hot vectors for all categories of a variable
def cat_to_dat(dtf, col, years, year_sensitive):
  # one hot vectors over years in years
 if year_sensitive:
   return (dtf[dtf.data_year.isin(years)])[col].astype(str).str.get_dummies(sep = ";")
  # one hot vectors over all years available
 else:
   return dtf[col].astype(str).str.get_dummies(sep = ";")

# returns the frequencies of a column's values
def get_cat_val_counts(dtf):
  total_sum = dtf.sum().sum()
  for col in dtf.columns:
    col_sum = dtf[col].sum()
    print(col + ": " + str(col_sum) + ' (' + str(round((col_sum/total_sum)* 100, 2)) + '%)')
  print('------------------------------------------')

for col in dtf.columns:
  print(col + "\n")
  get_cat_val_counts(cat_to_dat(dtf, col, years = [], year_sensitive = False))

"""Frequencies Before Cleaning"""

col = 'bias_desc'
cat_dat = cat_to_dat(dtf, col, [], False)
plot = cat_dat[cat_dat.columns].sum().nlargest(5)#.plot.barh()

bias_names = plot.keys()
bias_counts = plot.values

fig, ax = plt.subplots()
bar_container = ax.barh(bias_names, bias_counts)
ax.set(ylabel='Bias Description', title="Top 5 Bias Description Frequencies")
ax.set(xlabel='Frequency')
ax.bar_label(bar_container, fmt='{:,.0f}')

## + col.title().replace('_', ' ')

"""**Cleaning**"""

# just take the first value if a row has multiple values for a single column
dtf['offense_name'] = dtf['offense_name'].str.split(';').str[0]
dtf['location_name'] = dtf['location_name'].str.split(';').str[0]
dtf['victim_types'] = dtf['victim_types'].str.split(';').str[0]

# only keep the 50 states plus D.C. plus federal
remove_states = ['Guam']
dtf = dtf[~dtf.state_name.isin(remove_states)]

dtf['state_name'] = dtf['state_name'].replace('Federal', 'Federal (State)')

# only keep the 9 U.S. divisions
# remove_divisions = ['Other', 'U.S. Territories']
# dtf = dtf[~dtf.division_name.isin(remove_divisions)]

dtf['division_name'] = dtf['division_name'].replace('Other', 'Federal (Division)')
dtf['division_name'] = dtf['division_name'].replace('U.S. Territories', 'U.S. Territories (Division)')

# only keep the 4 U.S. regions
# remove_regions = ['Other', 'U.S. Territories']
# dtf = dtf[~dtf.region_name.isin(remove_regions)]

dtf['region_name'] = dtf['region_name'].replace('Other', 'Federal (Region)')
dtf['region_name'] = dtf['region_name'].replace('U.S. Territories', 'U.S. Territories (Region)')

# remove State Police densities
# remove_densities = ['MSA State Police', 'Non-MSA State Police']
# dtf = dtf[~dtf.population_group_description.isin(remove_densities)]

# remove rows that have a null value for population densities
dtf = dtf[dtf.population_group_description.notnull()]

# keep relevant offenses
# keep_offenses = ['Aggravated Assault', 'Arson', 'Burglary/Breaking & Entering',
#                 'Destruction/Damage/Vandalism of Property', 'Intimidation',
#                 'Kidnapping/Abduction', 'Murder and Nonnegligent Manslaughter',
#                 'Rape', 'Robbery', 'Simple Assault', 'All Other Larceny',
#                 'Stolen Property Offenses', 'Theft From Building',
#                 'Theft From Motor Vehicle',
#                 'Theft of Motor Vehicle Parts or Accessories',
#                 'Motor Vehicle Theft']
# dtf = dtf[dtf.offense_name.isin(keep_offenses)]

# group larceny and theft offenses
# larceny_theft = ['All Other Larceny', 'Stolen Property Offenses',
#                 'Theft From Building', 'Theft From Motor Vehicle',
#                 'Theft of Motor Vehicle Parts or Accessories',
#                'Motor Vehicle Theft']
# for offense in larceny_theft:
#  dtf['offense_name'] = dtf['offense_name'].replace(offense, 'Larceny/Theft')

# remove sparse offenses
threshold = 0.0001 * dtf.shape[0]

offense_dtf = cat_to_dat(dtf, 'offense_name', years = [], year_sensitive = False)
offenses_to_remove = []

for col in offense_dtf.columns:
  if offense_dtf[col].sum() < threshold:
    offenses_to_remove.append(str(col))

dtf = dtf[~dtf.offense_name.isin(offenses_to_remove)]

# remove null total individual victim counts
dtf = dtf[dtf.total_individual_victims.notnull()]

# remove sparse locations
threshold = 0.0001 * dtf.shape[0]

location_dtf = cat_to_dat(dtf, 'location_name', years = [], year_sensitive = False)
locations_to_remove = []

for col in location_dtf.columns:
    if location_dtf[col].sum() < threshold:
      locations_to_remove.append(str(col))

dtf = dtf[~dtf.location_name.isin(locations_to_remove)]

dtf['location_name'] = dtf['location_name'].replace("Other/Unknown", 'Other/Unknown Location Type')

# remove unknown offender race
# remove_races = ['Unknown', 'Not Specified']
# dtf = dtf[~dtf.offender_race.isin(remove_races)]

dtf['offender_race'] = dtf['offender_race'].replace("Unknown", 'Unknown Race')
dtf['offender_race'] = dtf['offender_race'].replace("Not Specified", 'Unknown Race')
dtf['offender_race'] = dtf['offender_race'].replace("Multiple", 'Mixed Race')

# remove rows with unknown biases
dtf = dtf[dtf.bias_desc != "Unknown (offender's motivation not known)"]

# remove unknown victim types
#remove_victim_types = ['Unknown', 'Other']
# dtf = dtf[~dtf.victim_types.isin(remove_victim_types)]

dtf['victim_types'] = dtf['victim_types'].replace("Unknown", 'Unknown Victim Type')
dtf['victim_types'] = dtf['victim_types'].replace("Other", 'Other Victim Type')

# remove rows with multiple biases
dtf = dtf[dtf.multiple_bias != "M"]

# remove columns that are too empty or unnecessary
dtf = dtf.drop(columns=['offender_ethnicity', 'multiple_offense', 'multiple_bias'])

"""Create classification classes."""

# group biases for more general classification
anti_disability = ['Anti-Mental Disability', 'Anti-Physical Disability']

anti_gender = ['Anti-Female', 'Anti-Male']

anti_gender_identity = ['Anti-Gender Non-Conforming', 'Anti-Transgender']

anti_race_ethnicity_ancestry = ['Anti-American Indian or Alaska Native',
                                'Anti-Arab', 'Anti-Asian', 'Anti-Black or African American',
                                'Anti-Native Hawaiian or Other Pacific Islander',
                                'Anti-Hispanic or Latino', 'Anti-Multiple Races, Group',
                                'Anti-Other Race/Ethnicity/Ancestry', 'Anti-White']

anti_religion = ['Anti-Atheism/Agnosticism', 'Anti-Buddhist', 'Anti-Catholic',
                 'Anti-Eastern Orthodox (Russian, Greek, Other)', 'Anti-Hindu',
                 'Anti-Islamic (Muslim)', "Anti-Jehovah's Witness", 'Anti-Jewish',
                 'Anti-Mormon', 'Anti-Multiple Religions, Group', 'Anti-Other Christian',
                 'Anti-Other Religion', 'Anti-Protestant', 'Anti-Sikh', 'Anti-Church of Jesus Christ']

anti_sexual_orientation = ['Anti-Bisexual', 'Anti-Gay (Male)',
                  'Anti-Lesbian, Gay, Bisexual, or Transgender (Mixed Group)',
                  'Anti-Heterosexual', 'Anti-Lesbian (Female)']

bias_groups = [anti_disability, anti_gender, anti_gender_identity,
               anti_race_ethnicity_ancestry, anti_religion, anti_sexual_orientation]

bias_groups_str = ['Anti-Disability', 'Anti-Gender', 'Anti-Gender Identity',
                   'Anti-Race/Ethnicity/Ancestry', 'Anti-Religion',
                   'Anti-Sexual Orientation']

for index, bias_group in enumerate(bias_groups):
  for bias in bias_group:
    if index == 0:
          dtf['bias_desc'] = dtf['bias_desc'].replace(bias,'Anti-Disability')
    elif index == 1:
          dtf['bias_desc'] = dtf['bias_desc'].replace(bias,'Anti-Gender')
    elif index == 2:
          dtf['bias_desc'] = dtf['bias_desc'].replace(bias,'Anti-Gender Identity')
    elif index == 3:
          dtf['bias_desc'] = dtf['bias_desc'].replace(bias,'Anti-Race/Ethnicity/Ancestry')
    elif index == 4:
          dtf['bias_desc'] = dtf['bias_desc'].replace(bias,'Anti-Religion')
    elif index == 5:
          dtf['bias_desc'] = dtf['bias_desc'].replace(bias,'Anti-Sexual Orientation')

"""Missing Value Imputation"""

# always at least 1 offender (impute to mode)
dtf.loc[dtf['total_offender_count'] == 0, 'total_offender_count'] = 1

dtf = dtf.reset_index(drop = "True")

# re-index the rows
dtf = dtf.reset_index(drop = "True")

# look at new frequencies
for col in dtf.columns:
  print(col + "\n")
  get_cat_val_counts(cat_to_dat(dtf, col, years = [], year_sensitive = False))

for bias_group in bias_groups_str:
  temp_dtf = dtf[dtf.bias_desc == bias_group]
  print('-----------------------------------')
  print(bias_group, ':')
  for col in temp_dtf.columns:
    print(col + ": ", temp_dtf[col].value_counts(normalize = 'true')[:3].index.tolist())

"""**Visualize data.**

Bar Plots for Frequency
"""

col = 'bias_desc'
cat_dat = cat_to_dat(dtf, col, [], False)
plot = cat_dat[cat_dat.columns].sum().nlargest(10)#.plot.barh()

bias_names = plot.keys()
bias_counts = plot.values

fig, ax = plt.subplots()
bar_container = ax.barh(bias_names, bias_counts)
ax.set(ylabel='Bias Description', title='General Bias Description Frequencies')
ax.set(xlabel='Frequency')
ax.bar_label(bar_container, fmt='{:,.0f}')

"""Line Plots for Frequency Over Time"""

# returns a dataframe of a column's category sums for each year
def cat_to_dat_over_time(dtf, col):
 one_hot_dtf = cat_to_dat(dtf, col, [], False)
 return pd.concat([dtf["data_year"], one_hot_dtf], axis = 1).groupby(['data_year']).sum()

col = 'division_name'
feature_to_plot_over_time = cat_to_dat_over_time(dtf, col)

# plot top 10 categories in frequency on line graph
for category in feature_to_plot_over_time.sum().nlargest(10).keys():
  plt.plot(feature_to_plot_over_time[category], label = category)

legend = plt.legend(loc = 'upper left', fontsize = "x-small")
plt.xlabel('Year')
plt.ylabel('Frequency')
plt.title(col.title().replace('_', ' '))
plt.show()

"""**Prepare data for training!**

Split the data into training and testing sets.
"""

y = dtf.bias_desc
X = dtf.drop(['bias_desc'], axis = 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 100, stratify = y)

"""Group Sparse Numerical Columns and Normalize Numerical Columns"""

# replace numerical outliers with median
median_tiv_gt_10 = (X_train.loc[X_train['total_individual_victims'] > 10, 'total_individual_victims']).median()
X_train.loc[X_train['total_individual_victims'] > 10, 'total_individual_victims'] = median_tiv_gt_10

median_vc_gt_10 = (X_train.loc[X_train['victim_count'] > 10, 'victim_count']).median()
X_train.loc[X_train['victim_count'] > 10, 'victim_count'] = median_vc_gt_10

median_toc_gt_10 = (X_train.loc[X_train['total_offender_count'] > 10, 'total_offender_count']).median()
X_train.loc[X_train['total_offender_count'] > 10, 'total_offender_count'] = median_toc_gt_10

# normalize
mms1 = MinMaxScaler()
X_train[['total_individual_victims','victim_count', 'total_offender_count']] = mms1.fit_transform(X_train[['total_individual_victims','victim_count', 'total_offender_count']])

# replace numerical outliers with median
median_tiv_gt_10 = (X_test.loc[X_test['total_individual_victims'] > 10, 'total_individual_victims']).median()
X_test.loc[X_test['total_individual_victims'] > 10, 'total_individual_victims'] = median_tiv_gt_10

median_vc_gt_10 = (X_test.loc[X_test['victim_count'] > 10, 'victim_count']).median()
X_test.loc[X_test['victim_count'] > 10, 'victim_count'] = median_vc_gt_10

median_toc_gt_10 = (X_test.loc[X_test['total_offender_count'] > 10, 'total_offender_count']).median()
X_test.loc[X_test['total_offender_count'] > 10, 'total_offender_count'] = median_toc_gt_10

# normalize
mms2 = MinMaxScaler()
X_test[['total_individual_victims','victim_count', 'total_offender_count']] = mms2.fit_transform(X_test[['total_individual_victims','victim_count', 'total_offender_count']])

"""**Feature Selection**"""

# calculates Cramerâ€™s V that is a measure of correlation that follows chi^2 test
def chi_square_test(col):
  cont_table = pd.crosstab(index=X_train[col], columns = y_train)
  chi2_test = scipy.stats.chi2_contingency(cont_table)
  chi2, p = chi2_test[0], chi2_test[1]
  n = cont_table.sum().sum()
  phi2 = chi2/n
  r, k = cont_table.shape
  phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))
  rcorr = r-((r-1)**2)/(n-1)
  kcorr = k-((k-1)**2)/(n-1)
  coeff = np.sqrt(phi2corr/min((kcorr-1), (rcorr-1)))
  coeff, p = round(coeff, 3), round(p, 3)
  conclusion = "Significant" if p < 0.05 else "Non-Significant"
  print("Cramer Correlation for", col, ":", coeff, conclusion, "(p-value:"+str(p)+")")

for col in X_train.columns:
    chi_square_test(col)

for dtf_X in [X_train, X_test]:
  print("Number of Unique Values for Columns Adjusted")
  print("-----------------------------------")
  for col in dtf_X.columns:
    print(col + ": " + str(dtf[col].nunique()))

# look at new frequencies
for col in dtf.columns:
  print(col + "\n")
  get_cat_val_counts(cat_to_dat(dtf, col, years = [], year_sensitive = False))

"""Create final dataframes for the training and testing sets."""

# create aggregate X dataframe with all categorical variables turned into one hot vectors
numerical_columns = ['total_offender_count', 'victim_count', 'total_individual_victims']

# X_train
new_dtf = pd.DataFrame()
for col in X_train.columns:
  if col not in numerical_columns:
    new_dtf = pd.concat([new_dtf, pd.get_dummies(X_train[col], drop_first = True)], axis = 1)
X_train = pd.concat([X_train[numerical_columns], new_dtf], axis = 1)

# X_test
new_dtf = pd.DataFrame()
for col in X_test.columns:
  if col not in numerical_columns:
    new_dtf = pd.concat([new_dtf, pd.get_dummies(X_test[col], drop_first = True)], axis = 1)
X_test = pd.concat([X_test[numerical_columns], new_dtf], axis = 1)

# make sure year columns are strings
X_train.columns = X_train.columns.astype(str)
X_test.columns = X_test.columns.astype(str)

X_train.info()
X_test.info()

"""Feature Selection (cont.)"""

feature_selector = SelectKBest(chi2, k = 25)
feature_fit = feature_selector.fit(X_train, y_train)
X_new = feature_fit.transform(X_train) # not needed to get the score
scores = feature_fit.scores_
print(X_new.shape)

X_train.columns[feature_selector.get_support()]

print(scores[:20])

"""Imbalanced Sampling"""

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state = 100, sampling_strategy = 'not majority')
X_train, y_train = smote.fit_resample(X_train, y_train)

y_train.value_counts()

X_train.info()
y_train.info()
X_test.info()
y_test.info()