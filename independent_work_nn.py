# -*- coding: utf-8 -*-
"""Independent Work NN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xWXXq9qQ7tkE2N_v0vcgkI9w_JECBnuB

# **Predicting Hate Crime Biases Using Multiclass Classification Machine Learning Models**

**Setup**

Import the necessary libraries.
"""

## for data manipulation
import pandas as pd
import numpy as np

## for plotting
import matplotlib.pyplot as plt

## for grouping sparse values
from statistics import median

## to normalize numerical features
from sklearn.preprocessing import MinMaxScaler

## for splitting the data
from sklearn.model_selection import train_test_split

## for evaluation metrics
from sklearn.metrics import classification_report, ConfusionMatrixDisplay, balanced_accuracy_score

## for neural network training
from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition
from sklearn.preprocessing import OneHotEncoder

import os
import shutil
import pickle as pk

from keras import models
from keras import layers
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import load_model
from keras import regularizers

"""# **Data Collection, Analysis, and Preparation**

Read the hate crime data into a pandas dataframe.
"""

dtf = pd.read_csv('hate_crime.csv')
dtf = dtf.sort_values(by=['incident_id'])

"""Remove preliminary features and data points that will not be needed."""

# remove duplicate rows
dtf.drop_duplicates(inplace = True)

# remove columns that are duplicates or unnecessary
dtf = dtf.drop(columns=['incident_id', 'ori', 'pug_agency_name',
                        'pub_agency_unit', 'agency_type_name',
                        'state_abbr', 'population_group_code',
                        'incident_date', 'adult_victim_count',
                        'juvenile_victim_count', 'adult_offender_count',
                        'juvenile_offender_count'])

"""Categorical Variable to Dataframe Function"""

# returns a dataframe of one hot vectors for all categories of a variable
def cat_to_dat(dtf, col, years, year_sensitive):
  # one hot vectors over years in years
 if year_sensitive:
   return (dtf[dtf.data_year.isin(years)])[col].astype(str).str.get_dummies(sep = ";")
  # one hot vectors over all years available
 else:
   return dtf[col].astype(str).str.get_dummies(sep = ";")

"""**Cleaning**"""

# just take the first value if a row has multiple values for a single column
dtf['offense_name'] = dtf['offense_name'].str.split(';').str[0]
dtf['location_name'] = dtf['location_name'].str.split(';').str[0]
dtf['victim_types'] = dtf['victim_types'].str.split(';').str[0]

# only keep the 50 states plus D.C. plus federal
remove_states = ['Guam']
dtf = dtf[~dtf.state_name.isin(remove_states)]

dtf['state_name'] = dtf['state_name'].replace('Federal', 'Federal (State)')

# only keep the 9 U.S. divisions
# remove_divisions = ['Other', 'U.S. Territories']
# dtf = dtf[~dtf.division_name.isin(remove_divisions)]

dtf['division_name'] = dtf['division_name'].replace('Other', 'Federal (Division)')
dtf['division_name'] = dtf['division_name'].replace('U.S. Territories', 'U.S. Territories (Division)')

# only keep the 4 U.S. regions
# remove_regions = ['Other', 'U.S. Territories']
# dtf = dtf[~dtf.region_name.isin(remove_regions)]

dtf['region_name'] = dtf['region_name'].replace('Other', 'Federal (Region)')
dtf['region_name'] = dtf['region_name'].replace('U.S. Territories', 'U.S. Territories (Region)')

# remove State Police densities
# remove_densities = ['MSA State Police', 'Non-MSA State Police']
# dtf = dtf[~dtf.population_group_description.isin(remove_densities)]

# remove rows that have a null value for population densities
dtf = dtf[dtf.population_group_description.notnull()]

# keep relevant offenses
# keep_offenses = ['Aggravated Assault', 'Arson', 'Burglary/Breaking & Entering',
#                 'Destruction/Damage/Vandalism of Property', 'Intimidation',
#                 'Kidnapping/Abduction', 'Murder and Nonnegligent Manslaughter',
#                 'Rape', 'Robbery', 'Simple Assault', 'All Other Larceny',
#                 'Stolen Property Offenses', 'Theft From Building',
#                 'Theft From Motor Vehicle',
#                 'Theft of Motor Vehicle Parts or Accessories',
#                 'Motor Vehicle Theft']
# dtf = dtf[dtf.offense_name.isin(keep_offenses)]

# group larceny and theft offenses
# larceny_theft = ['All Other Larceny', 'Stolen Property Offenses',
#                 'Theft From Building', 'Theft From Motor Vehicle',
#                 'Theft of Motor Vehicle Parts or Accessories',
#                'Motor Vehicle Theft']
# for offense in larceny_theft:
#  dtf['offense_name'] = dtf['offense_name'].replace(offense, 'Larceny/Theft')

# remove sparse offenses
threshold = 0.0001 * dtf.shape[0]

offense_dtf = cat_to_dat(dtf, 'offense_name', years = [], year_sensitive = False)
offenses_to_remove = []

for col in offense_dtf.columns:
  if offense_dtf[col].sum() < threshold:
    offenses_to_remove.append(str(col))

dtf = dtf[~dtf.offense_name.isin(offenses_to_remove)]

# remove null total individual victim counts
dtf = dtf[dtf.total_individual_victims.notnull()]

# remove sparse locations
threshold = 0.0001 * dtf.shape[0]

location_dtf = cat_to_dat(dtf, 'location_name', years = [], year_sensitive = False)
locations_to_remove = []

for col in location_dtf.columns:
    if location_dtf[col].sum() < threshold:
      locations_to_remove.append(str(col))

dtf = dtf[~dtf.location_name.isin(locations_to_remove)]

dtf['offender_race'] = dtf['offender_race'].replace("Other/Unknown", 'Other/Unknown Location Type')

# remove unknown offender race
# remove_races = ['Unknown', 'Not Specified']
# dtf = dtf[~dtf.offender_race.isin(remove_races)]

dtf['offender_race'] = dtf['offender_race'].replace("Unknown", 'Unknown Race')
dtf['offender_race'] = dtf['offender_race'].replace("Not Specified", 'Unknown Race')
dtf['offender_race'] = dtf['offender_race'].replace("Multiple", 'Mixed Race')

# remove rows with unknown biases
dtf = dtf[dtf.bias_desc != "Unknown (offender's motivation not known)"]

# remove unknown victim types
#remove_victim_types = ['Unknown', 'Other']
# dtf = dtf[~dtf.victim_types.isin(remove_victim_types)]

dtf['victim_types'] = dtf['victim_types'].replace("Unknown", 'Unknown Victim Type')
dtf['victim_types'] = dtf['victim_types'].replace("Other", 'Other Victim Type')

# remove rows with multiple biases
dtf = dtf[dtf.multiple_bias != "M"]

# remove columns that are too empty or unnecessary
dtf = dtf.drop(columns=['offender_ethnicity', 'multiple_offense', 'multiple_bias'])

"""Create classification classes."""

# group biases for more general classification
anti_disability = ['Anti-Mental Disability', 'Anti-Physical Disability']

anti_gender = ['Anti-Female', 'Anti-Male']

anti_gender_identity = ['Anti-Gender Non-Conforming', 'Anti-Transgender']

anti_race_ethnicity_ancestry = ['Anti-American Indian or Alaska Native',
                                'Anti-Arab', 'Anti-Asian', 'Anti-Black or African American',
                                'Anti-Native Hawaiian or Other Pacific Islander',
                                'Anti-Hispanic or Latino', 'Anti-Multiple Races, Group',
                                'Anti-Other Race/Ethnicity/Ancestry', 'Anti-White']

anti_religion = ['Anti-Atheism/Agnosticism', 'Anti-Buddhist', 'Anti-Catholic',
                 'Anti-Eastern Orthodox (Russian, Greek, Other)', 'Anti-Hindu',
                 'Anti-Islamic (Muslim)', "Anti-Jehovah's Witness", 'Anti-Jewish',
                 'Anti-Mormon', 'Anti-Multiple Religions, Group', 'Anti-Other Christian',
                 'Anti-Other Religion', 'Anti-Protestant', 'Anti-Sikh', 'Anti-Church of Jesus Christ']

anti_sexual_orientation = ['Anti-Bisexual', 'Anti-Gay (Male)',
                  'Anti-Lesbian, Gay, Bisexual, or Transgender (Mixed Group)',
                  'Anti-Heterosexual', 'Anti-Lesbian (Female)']

bias_groups = [anti_disability, anti_gender, anti_gender_identity,
               anti_race_ethnicity_ancestry, anti_religion, anti_sexual_orientation]

bias_groups_str = ['Anti-Disability', 'Anti-Gender', 'Anti-Gender Identity',
                   'Anti-Race/Ethnicity/Ancestry', 'Anti-Religion',
                   'Anti-Sexual Orientation']

for index, bias_group in enumerate(bias_groups):
  for bias in bias_group:
    if index == 0:
          dtf['bias_desc'] = dtf['bias_desc'].replace(bias,'Anti-Disability')
    elif index == 1:
          dtf['bias_desc'] = dtf['bias_desc'].replace(bias,'Anti-Gender')
    elif index == 2:
          dtf['bias_desc'] = dtf['bias_desc'].replace(bias,'Anti-Gender Identity')
    elif index == 3:
          dtf['bias_desc'] = dtf['bias_desc'].replace(bias,'Anti-Race/Ethnicity/Ancestry')
    elif index == 4:
          dtf['bias_desc'] = dtf['bias_desc'].replace(bias,'Anti-Religion')
    elif index == 5:
          dtf['bias_desc'] = dtf['bias_desc'].replace(bias,'Anti-Sexual Orientation')

"""Missing Value Imputation"""

# always at least 1 offender (impute to mode)
dtf.loc[dtf['total_offender_count'] == 0, 'total_offender_count'] = 1

# re-index the rows
dtf = dtf.reset_index(drop = "True")

"""**Prepare data for training!**

Split the data into training and testing sets.
"""

y = dtf.bias_desc
X = dtf.drop(['bias_desc'], axis = 1)

encoder = OneHotEncoder()

encoded_Y = encoder.fit(y.values.reshape(-1,1))
encoded_Y = encoded_Y.transform(y.values.reshape(-1,1)).toarray()

"""Split the data into training, validation, and testing sets."""

train_ratio = 0.80
validation_ratio = 0.10
test_ratio = 0.10

trainX, testX, trainY, testY = train_test_split(X, encoded_Y, test_size= 1 - train_ratio,  random_state = 100, stratify = encoded_Y)
valX, testX, valY, testY = train_test_split(testX, testY, test_size=test_ratio/(test_ratio + validation_ratio),  random_state = 100, stratify = testY)

"""Group Sparse Numerical Columns and Normalize Numerical Columns"""

# Training Data

# replace numerical outliers with median
median_tiv_gt_10 = (trainX.loc[trainX['total_individual_victims'] > 10, 'total_individual_victims']).median()
trainX.loc[trainX['total_individual_victims'] > 10, 'total_individual_victims'] = median_tiv_gt_10

median_vc_gt_10 = (trainX.loc[trainX['victim_count'] > 10, 'victim_count']).median()
trainX.loc[trainX['victim_count'] > 10, 'victim_count'] = median_vc_gt_10

median_toc_gt_10 = (trainX.loc[trainX['total_offender_count'] > 10, 'total_offender_count']).median()
trainX.loc[trainX['total_offender_count'] > 10, 'total_offender_count'] = median_toc_gt_10

# normalize
mms1 = MinMaxScaler()
trainX[['total_individual_victims','victim_count', 'total_offender_count']] = mms1.fit_transform(trainX[['total_individual_victims','victim_count', 'total_offender_count']])

# Validation Data

# replace numerical outliers with median
median_tiv_gt_10 = (valX.loc[valX['total_individual_victims'] > 10, 'total_individual_victims']).median()
valX.loc[valX['total_individual_victims'] > 10, 'total_individual_victims'] = median_tiv_gt_10

median_vc_gt_10 = (valX.loc[valX['victim_count'] > 10, 'victim_count']).median()
valX.loc[valX['victim_count'] > 10, 'victim_count'] = median_vc_gt_10

median_toc_gt_10 = (valX.loc[valX['total_offender_count'] > 10, 'total_offender_count']).median()
valX.loc[valX['total_offender_count'] > 10, 'total_offender_count'] = median_toc_gt_10

# normalize
mms2 = MinMaxScaler()
valX[['total_individual_victims','victim_count', 'total_offender_count']] = mms2.fit_transform(valX[['total_individual_victims','victim_count', 'total_offender_count']])

#Testing Data

# replace numerical outliers with median
median_tiv_gt_10 = (testX.loc[testX['total_individual_victims'] > 10, 'total_individual_victims']).median()
testX.loc[testX['total_individual_victims'] > 10, 'total_individual_victims'] = median_tiv_gt_10

median_vc_gt_10 = (testX.loc[testX['victim_count'] > 10, 'victim_count']).median()
testX.loc[testX['victim_count'] > 10, 'victim_count'] = median_vc_gt_10

median_toc_gt_10 = (testX.loc[testX['total_offender_count'] > 10, 'total_offender_count']).median()
testX.loc[testX['total_offender_count'] > 10, 'total_offender_count'] = median_toc_gt_10

# normalize
mms3 = MinMaxScaler()
testX[['total_individual_victims','victim_count', 'total_offender_count']] = mms3.fit_transform(testX[['total_individual_victims','victim_count', 'total_offender_count']])

"""Create final dataframes for the training and testing sets."""

# create aggregate X dataframe with all categorical variables turned into one hot vectors
numerical_columns = ['total_offender_count', 'victim_count', 'total_individual_victims']

# trainX
new_dtf = pd.DataFrame()
for col in trainX.columns:
  if col not in numerical_columns:
    new_dtf = pd.concat([new_dtf, pd.get_dummies(trainX[col], drop_first = True)], axis = 1)
trainX = pd.concat([trainX[numerical_columns], new_dtf], axis = 1)

# valX
new_dtf = pd.DataFrame()
for col in valX.columns:
  if col not in numerical_columns:
    new_dtf = pd.concat([new_dtf, pd.get_dummies(valX[col], drop_first = True)], axis = 1)
valX = pd.concat([valX[numerical_columns], new_dtf], axis = 1)

# testX
new_dtf = pd.DataFrame()
for col in testX.columns:
  if col not in numerical_columns:
    new_dtf = pd.concat([new_dtf, pd.get_dummies(testX[col], drop_first = True)], axis = 1)
testX = pd.concat([testX[numerical_columns], new_dtf], axis = 1)

# make sure year columns are strings
trainX.columns = trainX.columns.astype(str)
valX.columns = valX.columns.astype(str)
testX.columns = testX.columns.astype(str)

print(trainX.shape)
print(valX.shape)
print(testX.shape)

"""Imbalanced Sampling"""

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state = 100, sampling_strategy = 'not majority')
trainX, trainY = smote.fit_resample(trainX, trainY)

print(trainX.shape)
print(valX.shape)
print(testX.shape)

"""# **Model Training and Evaluation**

Neural Network

Untuned Neural Network
"""

checkpoint = 2023
sizes = [4]
rates = []

test_accuracies = []

for size in sizes:
  checkpoint_no = f"ckpt_{checkpoint}_ANN"
  model_name = f"HC_ANN_2FC_F{size}_{size}_epoch_25_no_dropout"
  checkpoint += 1

  input_shape = trainX.shape[1]

  n_batch_size = 32

  n_steps_per_epoch = int(trainX.shape[0] / n_batch_size)
  n_validation_steps = int(valX.shape[0] / n_batch_size)
  n_test_steps = int(testX.shape[0] / n_batch_size)

  n_epochs = 25

  num_classes = trainY.shape[1]

  model = models.Sequential()
  model.add(layers.Dense(size, activation='relu', input_shape=(input_shape,)))
  model.add(layers.Dense(size, activation='relu'))
  model.add(layers.Dense(num_classes, activation='softmax'))

  model.summary()

  model.compile(loss='categorical_crossentropy',
                optimizer='adam',
                metrics=['categorical_accuracy'])

  # Prepare a directory to store all the checkpoints.
  checkpoint_dir = './'+ checkpoint_no
  if not os.path.exists(checkpoint_dir):
      os.makedirs(checkpoint_dir)

  keras_callbacks = [ModelCheckpoint(filepath = checkpoint_dir + '/' + model_name,
                                    monitor='val_loss', save_best_only=True, mode='auto')]

  history = model.fit(trainX,
                      trainY,
                      steps_per_epoch=n_steps_per_epoch,
                      epochs=n_epochs,
                      batch_size=n_batch_size,
                      validation_data=(valX, valY),
                      validation_steps=n_validation_steps,
                      callbacks=[keras_callbacks])

  y_pred = model.predict(testX)
  y_pred = np.argmax(y_pred, axis=1)

  testY_fixed = np.argmax(testY, axis=1)

  print(classification_report(testY_fixed, y_pred, digits = 4))

  disp = ConfusionMatrixDisplay.from_predictions(testY_fixed, y_pred, normalize = 'true',
  cmap=plt.cm.Blues, display_labels=bias_groups_str, xticks_rotation = 'vertical')
  fig = disp.figure_
  fig.suptitle('Artifical Neural Network Untuned')

  print('---------------------------------')

  hist_df = pd.DataFrame(history.history)
  hist_df['epoch'] = hist_df.index + 1
  cols = list(hist_df.columns)
  cols = [cols[-1]] + cols[:-1]
  hist_df = hist_df[cols]
  hist_df.to_csv(checkpoint_no + '/' + 'history_df_' + model_name + '.csv')
  hist_df.head()

  values_of_best_model = hist_df[hist_df.val_loss == hist_df.val_loss.min()]
  values_of_best_model

  class_assignment = dict(zip(y, encoded_Y))

  df_temp = pd.DataFrame([class_assignment], columns=class_assignment.keys())
  df_temp = df_temp.stack()
  df_temp = pd.DataFrame(df_temp).reset_index().drop(['level_0'], axis=1)
  df_temp.columns = ['Category', 'Allocated Number']

  df_temp.to_csv(checkpoint_no + '/' + 'class_assignment_df_' + model_name + '.csv')

  pk.dump(encoder, open(checkpoint_no + '/' + 'encoder.pkl', 'wb'))

  acc = history.history['categorical_accuracy']
  val_acc = history.history['val_categorical_accuracy']
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(1, len(acc) + 1)

  #plt.plot(epochs, acc, 'r', label='Training Accuracy')
  #plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')
  #plt.title('Training and Validation Accuracy')
  #plt.legend()

  #plt.figure()

  #plt.plot(epochs, loss, 'r', label='Training Loss')
  #plt.plot(epochs, val_loss, 'b', label='Validation Loss')
  #plt.title('Training and Validation Loss')
  #plt.legend()

  #plt.show()

  # Loading the automatically saved model
  model_reloaded = load_model(checkpoint_no + '/' + model_name)

  # Saving the best model in the correct path and format
  root_directory = os.getcwd()
  checkpoint_dir = os.path.join(root_directory, checkpoint_no)
  model_name_temp = os.path.join(checkpoint_dir, model_name + '.h5')
  model_reloaded.save(model_name_temp)

  # Deletion of the automatically created folder under Model Checkpoint File.
  folder_name_temp = os.path.join(checkpoint_dir, model_name)
  shutil.rmtree(folder_name_temp, ignore_errors=True)

  best_model = load_model(model_name_temp)

  test_loss, test_acc = best_model.evaluate(testX,
                                            testY,
                                            steps=n_test_steps)

sizes = [16]
rates = (x * 0.1 for x in [1, 5])

test_accuracies = []

for size in sizes:
  for rate in rates:
    checkpoint_no = f"ckpt_{checkpoint}_ANN"
    model_name = f"HC_ANN_2FC_F{size}_{size}_epoch_25_dropout_{rate}"
    checkpoint += 1

    input_shape = trainX.shape[1]

    n_batch_size = 32

    n_steps_per_epoch = int(trainX.shape[0] / n_batch_size)
    n_validation_steps = int(valX.shape[0] / n_batch_size)
    n_test_steps = int(testX.shape[0] / n_batch_size)

    n_epochs = 25

    num_classes = trainY.shape[1]

    model = models.Sequential()
    model.add(layers.Dense(size, activation='relu', input_shape=(input_shape,)))
    model.add(layers.Dropout(rate))
    model.add(layers.Dense(size, activation='relu'))
    model.add(layers.Dropout(rate))
    model.add(layers.Dense(num_classes, activation='softmax'))

    model.summary()

    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['categorical_accuracy'])

    # Prepare a directory to store all the checkpoints.
    checkpoint_dir = './'+ checkpoint_no
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)

    keras_callbacks = [ModelCheckpoint(filepath = checkpoint_dir + '/' + model_name,
                                      monitor='val_loss', save_best_only=True, mode='auto')]

    history = model.fit(trainX,
                        trainY,
                        steps_per_epoch=n_steps_per_epoch,
                        epochs=n_epochs,
                        batch_size=n_batch_size,
                        validation_data=(valX, valY),
                        validation_steps=n_validation_steps,
                        callbacks=[keras_callbacks])

    y_pred = model.predict(testX)
    y_pred = np.argmax(y_pred, axis=1)

    testY_fixed = np.argmax(testY, axis=1)

    print(classification_report(testY_fixed, y_pred, digits = 4))

    disp = ConfusionMatrixDisplay.from_predictions(testY_fixed, y_pred, normalize = 'true',
    cmap=plt.cm.Blues, display_labels=bias_groups_str, xticks_rotation = 'vertical')
    fig = disp.figure_
    fig.suptitle('Artifical Neural Network Tuned for Weighted F1')

    print('---------------------------------')

    hist_df = pd.DataFrame(history.history)
    hist_df['epoch'] = hist_df.index + 1
    cols = list(hist_df.columns)
    cols = [cols[-1]] + cols[:-1]
    hist_df = hist_df[cols]
    hist_df.to_csv(checkpoint_no + '/' + 'history_df_' + model_name + '.csv')
    hist_df.head()

    values_of_best_model = hist_df[hist_df.val_loss == hist_df.val_loss.min()]
    values_of_best_model

    class_assignment = dict(zip(y, encoded_Y))

    df_temp = pd.DataFrame([class_assignment], columns=class_assignment.keys())
    df_temp = df_temp.stack()
    df_temp = pd.DataFrame(df_temp).reset_index().drop(['level_0'], axis=1)
    df_temp.columns = ['Category', 'Allocated Number']

    df_temp.to_csv(checkpoint_no + '/' + 'class_assignment_df_' + model_name + '.csv')

    pk.dump(encoder, open(checkpoint_no + '/' + 'encoder.pkl', 'wb'))

    acc = history.history['categorical_accuracy']
    val_acc = history.history['val_categorical_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(acc) + 1)

    #plt.plot(epochs, acc, 'r', label='Training Accuracy')
    #plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')
    #plt.title('Training and Validation Accuracy')
    #plt.legend()

    #plt.figure()

    #plt.plot(epochs, loss, 'r', label='Training Loss')
    #plt.plot(epochs, val_loss, 'b', label='Validation Loss')
    #plt.title('Training and Validation Loss')
    #plt.legend()

    #plt.show()

    # Loading the automatically saved model
    model_reloaded = load_model(checkpoint_no + '/' + model_name)

    # Saving the best model in the correct path and format
    root_directory = os.getcwd()
    checkpoint_dir = os.path.join(root_directory, checkpoint_no)
    model_name_temp = os.path.join(checkpoint_dir, model_name + '.h5')
    model_reloaded.save(model_name_temp)

    # Deletion of the automatically created folder under Model Checkpoint File.
    folder_name_temp = os.path.join(checkpoint_dir, model_name)
    shutil.rmtree(folder_name_temp, ignore_errors=True)

    best_model = load_model(model_name_temp)

    test_loss, test_acc = best_model.evaluate(testX,
                                              testY,
                                              steps=n_test_steps)

sizes = [32]
rates = (x * 0.1 for x in [1, 5])

test_accuracies = []

for size in sizes:
  for rate in rates:
    checkpoint_no = f"ckpt_{checkpoint}_ANN"
    model_name = f"HC_ANN_2FC_F{size}_{size}_epoch_25_dropout_{rate}"
    checkpoint += 1

    input_shape = trainX.shape[1]

    n_batch_size = 32

    n_steps_per_epoch = int(trainX.shape[0] / n_batch_size)
    n_validation_steps = int(valX.shape[0] / n_batch_size)
    n_test_steps = int(testX.shape[0] / n_batch_size)

    n_epochs = 25

    num_classes = trainY.shape[1]

    model = models.Sequential()
    model.add(layers.Dense(size, activation='relu', input_shape=(input_shape,)))
    model.add(layers.Dropout(rate))
    model.add(layers.Dense(size, activation='relu'))
    model.add(layers.Dropout(rate))
    model.add(layers.Dense(num_classes, activation='softmax'))

    model.summary()

    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['categorical_accuracy'])

    # Prepare a directory to store all the checkpoints.
    checkpoint_dir = './'+ checkpoint_no
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)

    keras_callbacks = [ModelCheckpoint(filepath = checkpoint_dir + '/' + model_name,
                                      monitor='val_loss', save_best_only=True, mode='auto')]

    history = model.fit(trainX,
                        trainY,
                        steps_per_epoch=n_steps_per_epoch,
                        epochs=n_epochs,
                        batch_size=n_batch_size,
                        validation_data=(valX, valY),
                        validation_steps=n_validation_steps,
                        callbacks=[keras_callbacks])

    y_pred = model.predict(testX)
    y_pred = np.argmax(y_pred, axis=1)

    testY_fixed = np.argmax(testY, axis=1)

    print(classification_report(testY_fixed, y_pred, digits = 4))

    disp = ConfusionMatrixDisplay.from_predictions(testY_fixed, y_pred, normalize = 'true',
    cmap=plt.cm.Blues, display_labels=bias_groups_str, xticks_rotation = 'vertical')
    fig = disp.figure_
    fig.suptitle('Artifical Neural Network Tuned for Weighted F1')

    print('---------------------------------')

    hist_df = pd.DataFrame(history.history)
    hist_df['epoch'] = hist_df.index + 1
    cols = list(hist_df.columns)
    cols = [cols[-1]] + cols[:-1]
    hist_df = hist_df[cols]
    hist_df.to_csv(checkpoint_no + '/' + 'history_df_' + model_name + '.csv')
    hist_df.head()

    values_of_best_model = hist_df[hist_df.val_loss == hist_df.val_loss.min()]
    values_of_best_model

    class_assignment = dict(zip(y, encoded_Y))

    df_temp = pd.DataFrame([class_assignment], columns=class_assignment.keys())
    df_temp = df_temp.stack()
    df_temp = pd.DataFrame(df_temp).reset_index().drop(['level_0'], axis=1)
    df_temp.columns = ['Category', 'Allocated Number']

    df_temp.to_csv(checkpoint_no + '/' + 'class_assignment_df_' + model_name + '.csv')

    pk.dump(encoder, open(checkpoint_no + '/' + 'encoder.pkl', 'wb'))

    acc = history.history['categorical_accuracy']
    val_acc = history.history['val_categorical_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(acc) + 1)

    #plt.plot(epochs, acc, 'r', label='Training Accuracy')
    #plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')
    #plt.title('Training and Validation Accuracy')
    #plt.legend()

    #plt.figure()

    #plt.plot(epochs, loss, 'r', label='Training Loss')
    #plt.plot(epochs, val_loss, 'b', label='Validation Loss')
    #plt.title('Training and Validation Loss')
    #plt.legend()

    #plt.show()

    # Loading the automatically saved model
    model_reloaded = load_model(checkpoint_no + '/' + model_name)

    # Saving the best model in the correct path and format
    root_directory = os.getcwd()
    checkpoint_dir = os.path.join(root_directory, checkpoint_no)
    model_name_temp = os.path.join(checkpoint_dir, model_name + '.h5')
    model_reloaded.save(model_name_temp)

    # Deletion of the automatically created folder under Model Checkpoint File.
    folder_name_temp = os.path.join(checkpoint_dir, model_name)
    shutil.rmtree(folder_name_temp, ignore_errors=True)

    best_model = load_model(model_name_temp)

    test_loss, test_acc = best_model.evaluate(testX,
                                              testY,
                                              steps=n_test_steps)

sizes = [64]
rates = (x * 0.1 for x in [1, 5])

test_accuracies = []

for size in sizes:
  for rate in rates:
    checkpoint_no = f"ckpt_{checkpoint}_ANN"
    model_name = f"HC_ANN_2FC_F{size}_{size}_epoch_25_dropout_{rate}"
    checkpoint += 1

    input_shape = trainX.shape[1]

    n_batch_size = 32

    n_steps_per_epoch = int(trainX.shape[0] / n_batch_size)
    n_validation_steps = int(valX.shape[0] / n_batch_size)
    n_test_steps = int(testX.shape[0] / n_batch_size)

    n_epochs = 25

    num_classes = trainY.shape[1]

    model = models.Sequential()
    model.add(layers.Dense(size, activation='relu', input_shape=(input_shape,)))
    model.add(layers.Dropout(rate))
    model.add(layers.Dense(size, activation='relu'))
    model.add(layers.Dropout(rate))
    model.add(layers.Dense(num_classes, activation='softmax'))

    model.summary()

    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['categorical_accuracy'])

    # Prepare a directory to store all the checkpoints.
    checkpoint_dir = './'+ checkpoint_no
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)

    keras_callbacks = [ModelCheckpoint(filepath = checkpoint_dir + '/' + model_name,
                                      monitor='val_loss', save_best_only=True, mode='auto')]

    history = model.fit(trainX,
                        trainY,
                        steps_per_epoch=n_steps_per_epoch,
                        epochs=n_epochs,
                        batch_size=n_batch_size,
                        validation_data=(valX, valY),
                        validation_steps=n_validation_steps,
                        callbacks=[keras_callbacks])

    y_pred = model.predict(testX)
    y_pred = np.argmax(y_pred, axis=1)

    testY_fixed = np.argmax(testY, axis=1)

    print(classification_report(testY_fixed, y_pred, digits = 4))

    disp = ConfusionMatrixDisplay.from_predictions(testY_fixed, y_pred, normalize = 'true',
    cmap=plt.cm.Blues, display_labels=bias_groups_str, xticks_rotation = 'vertical')
    fig = disp.figure_
    fig.suptitle('Artifical Neural Network Tuned for Weighted F1')

    print('---------------------------------')

    hist_df = pd.DataFrame(history.history)
    hist_df['epoch'] = hist_df.index + 1
    cols = list(hist_df.columns)
    cols = [cols[-1]] + cols[:-1]
    hist_df = hist_df[cols]
    hist_df.to_csv(checkpoint_no + '/' + 'history_df_' + model_name + '.csv')
    hist_df.head()

    values_of_best_model = hist_df[hist_df.val_loss == hist_df.val_loss.min()]
    values_of_best_model

    class_assignment = dict(zip(y, encoded_Y))

    df_temp = pd.DataFrame([class_assignment], columns=class_assignment.keys())
    df_temp = df_temp.stack()
    df_temp = pd.DataFrame(df_temp).reset_index().drop(['level_0'], axis=1)
    df_temp.columns = ['Category', 'Allocated Number']

    df_temp.to_csv(checkpoint_no + '/' + 'class_assignment_df_' + model_name + '.csv')

    pk.dump(encoder, open(checkpoint_no + '/' + 'encoder.pkl', 'wb'))

    acc = history.history['categorical_accuracy']
    val_acc = history.history['val_categorical_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(acc) + 1)

    #plt.plot(epochs, acc, 'r', label='Training Accuracy')
    #plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')
    #plt.title('Training and Validation Accuracy')
    #plt.legend()

    #plt.figure()

    #plt.plot(epochs, loss, 'r', label='Training Loss')
    #plt.plot(epochs, val_loss, 'b', label='Validation Loss')
    #plt.title('Training and Validation Loss')
    #plt.legend()

    #plt.show()

    # Loading the automatically saved model
    model_reloaded = load_model(checkpoint_no + '/' + model_name)

    # Saving the best model in the correct path and format
    root_directory = os.getcwd()
    checkpoint_dir = os.path.join(root_directory, checkpoint_no)
    model_name_temp = os.path.join(checkpoint_dir, model_name + '.h5')
    model_reloaded.save(model_name_temp)

    # Deletion of the automatically created folder under Model Checkpoint File.
    folder_name_temp = os.path.join(checkpoint_dir, model_name)
    shutil.rmtree(folder_name_temp, ignore_errors=True)

    best_model = load_model(model_name_temp)

    test_loss, test_acc = best_model.evaluate(testX,
                                              testY,
                                              steps=n_test_steps)

sizes = [128]
rates = (x * 0.1 for x in [1, 5])

test_accuracies = []

for size in sizes:
  for rate in rates:
    checkpoint_no = f"ckpt_{checkpoint}_ANN"
    model_name = f"HC_ANN_2FC_F{size}_{size}_epoch_25_dropout_{rate}"
    checkpoint += 1

    input_shape = trainX.shape[1]

    n_batch_size = 32

    n_steps_per_epoch = int(trainX.shape[0] / n_batch_size)
    n_validation_steps = int(valX.shape[0] / n_batch_size)
    n_test_steps = int(testX.shape[0] / n_batch_size)

    n_epochs = 25

    num_classes = trainY.shape[1]

    model = models.Sequential()
    model.add(layers.Dense(size, activation='relu', input_shape=(input_shape,)))
    model.add(layers.Dropout(rate))
    model.add(layers.Dense(size, activation='relu'))
    model.add(layers.Dropout(rate))
    model.add(layers.Dense(num_classes, activation='softmax'))

    model.summary()

    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['categorical_accuracy'])

    # Prepare a directory to store all the checkpoints.
    checkpoint_dir = './'+ checkpoint_no
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)

    keras_callbacks = [ModelCheckpoint(filepath = checkpoint_dir + '/' + model_name,
                                      monitor='val_loss', save_best_only=True, mode='auto')]

    history = model.fit(trainX,
                        trainY,
                        steps_per_epoch=n_steps_per_epoch,
                        epochs=n_epochs,
                        batch_size=n_batch_size,
                        validation_data=(valX, valY),
                        validation_steps=n_validation_steps,
                        callbacks=[keras_callbacks])

    y_pred = model.predict(testX)
    y_pred = np.argmax(y_pred, axis=1)

    testY_fixed = np.argmax(testY, axis=1)

    print(classification_report(testY_fixed, y_pred, digits = 4))

    disp = ConfusionMatrixDisplay.from_predictions(testY_fixed, y_pred, normalize = 'true',
    cmap=plt.cm.Blues, display_labels=bias_groups_str, xticks_rotation = 'vertical')
    fig = disp.figure_
    fig.suptitle('Artifical Neural Network Tuned for Weighted F1')

    print('---------------------------------')

    hist_df = pd.DataFrame(history.history)
    hist_df['epoch'] = hist_df.index + 1
    cols = list(hist_df.columns)
    cols = [cols[-1]] + cols[:-1]
    hist_df = hist_df[cols]
    hist_df.to_csv(checkpoint_no + '/' + 'history_df_' + model_name + '.csv')
    hist_df.head()

    values_of_best_model = hist_df[hist_df.val_loss == hist_df.val_loss.min()]
    values_of_best_model

    class_assignment = dict(zip(y, encoded_Y))

    df_temp = pd.DataFrame([class_assignment], columns=class_assignment.keys())
    df_temp = df_temp.stack()
    df_temp = pd.DataFrame(df_temp).reset_index().drop(['level_0'], axis=1)
    df_temp.columns = ['Category', 'Allocated Number']

    df_temp.to_csv(checkpoint_no + '/' + 'class_assignment_df_' + model_name + '.csv')

    pk.dump(encoder, open(checkpoint_no + '/' + 'encoder.pkl', 'wb'))

    acc = history.history['categorical_accuracy']
    val_acc = history.history['val_categorical_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(acc) + 1)

    #plt.plot(epochs, acc, 'r', label='Training Accuracy')
    #plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')
    #plt.title('Training and Validation Accuracy')
    #plt.legend()

    #plt.figure()

    #plt.plot(epochs, loss, 'r', label='Training Loss')
    #plt.plot(epochs, val_loss, 'b', label='Validation Loss')
    #plt.title('Training and Validation Loss')
    #plt.legend()

    #plt.show()

    # Loading the automatically saved model
    model_reloaded = load_model(checkpoint_no + '/' + model_name)

    # Saving the best model in the correct path and format
    root_directory = os.getcwd()
    checkpoint_dir = os.path.join(root_directory, checkpoint_no)
    model_name_temp = os.path.join(checkpoint_dir, model_name + '.h5')
    model_reloaded.save(model_name_temp)

    # Deletion of the automatically created folder under Model Checkpoint File.
    folder_name_temp = os.path.join(checkpoint_dir, model_name)
    shutil.rmtree(folder_name_temp, ignore_errors=True)

    best_model = load_model(model_name_temp)

    test_loss, test_acc = best_model.evaluate(testX,
                                              testY,
                                              steps=n_test_steps)